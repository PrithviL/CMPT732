{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 #temp_range by Prithvi Lakshminarayanan 21-10-2016\
\
import sys\
from pyspark import SparkConf, SparkContext\
from pyspark.sql.context import SQLContext\
from pyspark.sql.types import StructType, StructField, StringType, DoubleType\
\
conf = SparkConf().setAppName("temp range")\
sc = SparkContext(conf=conf)\
sqlContext = SQLContext(sc)\
assert sc.version >= '1.5.1'\
\
inputs = sys.argv[1]\
output = sys.argv[2]\
\
temp_schema = StructType([\
    StructField('StationID', StringType(), False),\
    StructField('DateTime', StringType(), False),\
    StructField('Observation', StringType(), False),\
    StructField('DataValue', DoubleType(), False),\
    StructField('MFlag', StringType(), True),\
    StructField('QFlag', StringType(), True),\
    StructField('SFlag', StringType(), True),\
    StructField('OBSTime', StringType(), True),\
    ])\
\
df = sqlContext.read \\\
    .format('com.databricks.spark.csv') \\\
    .options(header='false') \\\
    .load(inputs, schema=temp_schema)\
df = df.filter(df.QFlag == '')\
\
dfmax = df.select(df['StationID'], df['DateTime'], df['Observation'], df['DataValue']).where(df['Observation'] == 'TMAX')\
dfmax = dfmax.withColumnRenamed('Observation', 'MaxObservation')\
dfmax = dfmax.withColumnRenamed('DataValue', 'MaxDataValue')\
\
dfmin = df.select(df['StationID'], df['DateTime'], df['Observation'], df['DataValue']).where(df['Observation'] == 'TMIN')\
dfmin = dfmin.withColumnRenamed('Observation', 'MinObservation')\
dfmin = dfmin.withColumnRenamed('DataValue', 'MinDataValue')\
\
cond1 = [dfmax['StationID'] == dfmin['StationID'], dfmax['DateTime'] == dfmin['DateTime']]\
dfrange = dfmax.join(dfmin, cond1, 'inner').select(dfmax['StationID'], dfmax['DateTime'], (dfmax['MaxDataValue']-dfmin['MinDataValue']).alias('Range'))\
\
df_maxrange = dfrange.groupby('DateTime').agg(\{"Range": "max"\})\
df_maxrange = df_maxrange.withColumnRenamed('max(Range)', 'MaxRange')\
\
cond2 = [dfrange['DateTime'] == df_maxrange['DateTime'], dfrange['Range'] == df_maxrange['MaxRange']]\
df_result = dfrange.join(df_maxrange, cond2, 'inner').select(dfrange['DateTime'], dfrange['StationID'], df_maxrange['MaxRange'])\
\
rdd_result = df_result.rdd.sortBy(lambda r:r[0])\
results = rdd_result.map(lambda r: str(r.DateTime)+' '+str(r.StationID)+' '+str(r.MaxRange))\
results.saveAsTextFile(output)\
\
}