{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 #Prithvi Lakshminarayanan\
#3-11-2016\
\
from pyspark import SparkConf\
import pyspark_cassandra\
import sys\
from pyspark.sql import SQLContext\
\
def df_for_orders(sqlContext, keyspace, strkey, table, split_size=None):\
	rdd = (sc.cassandraTable(keyspace, table, split_size=split_size)\
			.select('orderkey', 'totalprice')\
			.where("orderkey in (" + strkey + ")")\
			.setName(table))\
	df = sqlContext.createDataFrame(rdd)\
	df.registerTempTable(table)\
	return df\
\
def df_for_lineitem(sqlContext, keyspace, strkey, table, split_size=None):\
	rdd = (sc.cassandraTable(keyspace, table, split_size=split_size)\
			.select('orderkey', 'partkey')\
			.where("orderkey in (" + strkey + ")")\
			.setName(table))\
	df = sqlContext.createDataFrame(rdd)\
	df.registerTempTable(table)\
	return df\
\
def df_for_part(sqlContext, keyspace, strkey, table, split_size=None):\
	rdd = (sc.cassandraTable(keyspace, table, split_size=split_size)\
			.select('name', 'partkey')\
			.setName(table))\
	df = sqlContext.createDataFrame(rdd)\
	df.registerTempTable(table)\
	return df\
\
def main(sc, sqlContext, keyspace, output, keys):\
	strkey = ",".join(keys)\
\
	df_for_orders(sqlContext, keyspace, strkey, "orders")\
	df_for_lineitem(sqlContext, keyspace, strkey, "lineitem")\
	df_for_part(sqlContext, keyspace, strkey, "part")\
	   \
	orders = (sqlContext.sql("""SELECT o.orderkey, o.totalprice, p.name FROM\
	                            orders o\
	                            JOIN lineitem l ON (o.orderkey = l.orderkey)\
	                            JOIN part p ON (l.partkey = p.partkey)"""))\
\
	orderRdd = (orders.rdd.map(lambda row: (str(row['orderkey']) + " $" + str(row['totalprice']), str(row['name'])))\
	                      .reduceByKey(lambda a,b : a + "," + b)).coalesce(1)\
\
	outdata =  orderRdd.map(lambda (key,value): "Order #" + str(key) +  ": " +  str(value))\
\
	outdata.saveAsTextFile(output)\
\
if __name__ == "__main__":\
    cluster_seeds = ['199.60.17.136', '199.60.17.173']     \
    conf = (SparkConf().setAppName('Tpch Orders SQL')\
    				  .set('spark.cassandra.connection.host', ','.join(cluster_seeds))\
    				  .set('spark.dynamicAllocation.maxExecutors', 20))\
    sc = pyspark_cassandra.CassandraSparkContext(conf=conf)\
    sqlContext = SQLContext(sc)\
    \
    keyspace = sys.argv[1]\
    output = sys.argv[2]\
    keys = sys.argv[3:]\
    \
  \
    main(sc, sqlContext, keyspace, output, keys)\
}