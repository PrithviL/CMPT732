{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf820
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 #Prithvi Lakshminarayanan\
#Euler\
import sys\
import random\
import operator\
from pyspark import SparkConf, SparkContext\
\
conf = SparkConf().setAppName("euler")\
sc = SparkContext(conf=conf)\
assert sc.version >= '1.5.1'\
\
total_iters = long(sys.argv[1])\
\
#iterating the values\
def do_things(iter):\
    count = 0\
    for i in range(iter):\
        sum = 0.0\
        while (sum < 1):\
            sum += random.random()\
            count += 1\
    return count\
\
\
def main():\
    partition_num = 2000  #2000 because number of machines multiplied by the number of cores\
    iteration = total_iters/partition_num\
\
    lst = []\
    for i in range(partition_num):\
        lst.append(iteration)\
\
#usind rdd\
    iter_rdd = sc.parallelize(lst, partition_num)\
    count_rdd = iter_rdd.map(do_things)\
    p = count_rdd.reduce(operator.add)\
    result = float(s)/total_iters\
    print p, total_iters, result\
\
if __name__ == "__main__":\
    main()\
}